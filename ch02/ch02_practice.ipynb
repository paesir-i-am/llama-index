{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e64f638-fa34-4ef7-a0ae-e4f3c3f62656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index-indices-managed-llama-cloud 0.6.0 requires llama-index-core<0.12.0,>=0.11.13.post1, but you have llama-index-core 0.11.11 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install llama-index==0.11.11 -q\n",
    "!pip install llama-index-core==0.11.11 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d511edf9-6b4d-4e7f-88b4-f6829dd72cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting docx2txt\n",
      "  Using cached docx2txt-0.9-py3-none-any.whl.metadata (529 bytes)\n",
      "Using cached docx2txt-0.9-py3-none-any.whl (4.0 kB)\n",
      "Installing collected packages: docx2txt\n",
      "Successfully installed docx2txt-0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install docx2txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca605ea-5889-4362-a99f-428cd0f0f349",
   "metadata": {},
   "source": [
    "## 2.2 데이터 로딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4d8a9e-1d9d-4f4e-a1a2-ea3ea9d87fe2",
   "metadata": {},
   "source": [
    "### 2.2.1 데이터 리더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52e12c8c-a9c1-467d-83f9-5075eb69dfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"sample_docs\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68b5a7cf-da7a-4f49-a8f7-1f364f82e688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "This is the content of file 1.\n",
      "\n",
      "\n",
      "Document 2:\n",
      "This is the content of file 2.\n",
      "\n",
      "\n",
      "Document 3:\n",
      "This is the content of ﬁle 3. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Document 리스트의 각 요소에 접근하여 내용 출력\n",
    "for i, document in enumerate(documents):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(document.text)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9c2ef93-2743-4f6c-84a8-1577b9d993e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\n",
    "    \"sample_docs\", recursive=True).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "592bdffb-8901-4244-8d02-385fa979e056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "This is the content of file 1.\n",
      "\n",
      "\n",
      "Document 2:\n",
      "This is the content of file 2.\n",
      "\n",
      "\n",
      "Document 3:\n",
      "This is the content of ﬁle 3. \n",
      "\n",
      "\n",
      "Document 4:\n",
      "This is the content of file 4.\n",
      "\n",
      "\n",
      "Document 5:\n",
      "This is the content of file 5.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Document 리스트의 각 요소에 접근하여 내용 출력\n",
    "for i, document in enumerate(documents):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(document.text)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "085eea22-bf75-4d39-bf44-85f7588b137c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\n",
    "    \"sample_docs\",\n",
    "    required_exts=[\".txt\", \".pdf\"],\n",
    "    recursive=True).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd589fcd-bafb-456e-9c45-a2190a2e070b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "This is the content of file 1.\n",
      "\n",
      "\n",
      "Document 2:\n",
      "This is the content of ﬁle 3. \n",
      "\n",
      "\n",
      "Document 3:\n",
      "This is the content of file 4.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Document 리스트의 각 요소에 접근하여 내용 출력\n",
    "for i, document in enumerate(documents):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(document.text)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29a7554c-8a98-47cf-aa07-eb1e191efc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 metadata:\n",
      "{'file_path': '/Users/paesir/Desktop/llama-index/ch02/sample_docs/file1.txt', 'file_name': 'file1.txt', 'file_type': 'text/plain', 'file_size': 30, 'creation_date': '2025-07-06', 'last_modified_date': '2025-07-06'}\n",
      "\n",
      "\n",
      "Document 2 metadata:\n",
      "{'page_label': '1', 'file_name': 'file3.pdf', 'file_path': '/Users/paesir/Desktop/llama-index/ch02/sample_docs/file3.pdf', 'file_type': 'application/pdf', 'file_size': 9283, 'creation_date': '2025-07-06', 'last_modified_date': '2025-07-06'}\n",
      "\n",
      "\n",
      "Document 3 metadata:\n",
      "{'file_path': '/Users/paesir/Desktop/llama-index/ch02/sample_docs/sub_sample_docs/file4.txt', 'file_name': 'file4.txt', 'file_type': 'text/plain', 'file_size': 30, 'creation_date': '2025-07-06', 'last_modified_date': '2025-07-06'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# document 리스트의 각 요소에 접근하여 메타데이터를 출력\n",
    "for i, document in enumerate(documents):\n",
    "    print(f\"Document {i+1} metadata:\")\n",
    "    print(document.metadata)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9a7247-09a8-4d1b-8713-d735e99491a5",
   "metadata": {},
   "source": [
    "### 2.2.2 데이터 커넥터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc393c4f-234e-47f1-9b76-573cc05c00da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index-question-gen-openai 0.2.0 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
      "llama-index 0.11.11 requires llama-index-core<0.12.0,>=0.11.10, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
      "llama-index-program-openai 0.2.0 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
      "llama-index-agent-openai 0.3.4 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
      "llama-index-multi-modal-llms-openai 0.2.3 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
      "llama-index-embeddings-openai 0.2.5 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
      "llama-index-cli 0.3.1 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
      "llama-index-readers-llama-parse 0.3.0 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
      "llama-index-readers-file 0.2.2 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
      "llama-index-llms-openai 0.2.16 requires llama-index-core<0.12.0,>=0.11.7, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
      "llama-index-indices-managed-llama-cloud 0.6.0 requires llama-index-core<0.12.0,>=0.11.13.post1, but you have llama-index-core 0.12.52.post1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install llama-index-readers-database==0.3.0 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9ed0e00-2c38-4383-8b74-84d88071868d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymysql\n",
      "  Using cached pymysql-1.1.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Using cached pymysql-1.1.2-py3-none-any.whl (45 kB)\n",
      "Installing collected packages: pymysql\n",
      "Successfully installed pymysql-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8d4ad44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cryptography\n",
      "  Using cached cryptography-45.0.7-cp311-abi3-macosx_10_9_universal2.whl.metadata (5.7 kB)\n",
      "Collecting cffi>=1.14 (from cryptography)\n",
      "  Using cached cffi-2.0.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.6 kB)\n",
      "Collecting pycparser (from cffi>=1.14->cryptography)\n",
      "  Using cached pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
      "Using cached cryptography-45.0.7-cp311-abi3-macosx_10_9_universal2.whl (7.0 MB)\n",
      "Using cached cffi-2.0.0-cp312-cp312-macosx_11_0_arm64.whl (181 kB)\n",
      "Using cached pycparser-2.23-py3-none-any.whl (118 kB)\n",
      "Installing collected packages: pycparser, cffi, cryptography\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [cryptography][0m [cryptography]\n",
      "\u001b[1A\u001b[2KSuccessfully installed cffi-2.0.0 cryptography-45.0.7 pycparser-2.23\n"
     ]
    }
   ],
   "source": [
    "  !pip install cryptography"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ecbf2c",
   "metadata": {},
   "source": [
    "## MySql 테이블 및 정보 추가 쿼리\n",
    "\n",
    "```sql\n",
    "-- 1) 데이터베이스 생성\n",
    "CREATE DATABASE IF NOT EXISTS test_db\n",
    "  DEFAULT CHARACTER SET utf8mb4\n",
    "  DEFAULT COLLATE utf8mb4_0900_ai_ci;\n",
    "\n",
    "-- 2) 로컬 전용 사용자 생성\n",
    "CREATE USER IF NOT EXISTS 'test'@'localhost' IDENTIFIED BY 'test';\n",
    "\n",
    "-- 3) 해당 DB에 대한 권한 부여\n",
    "GRANT ALL PRIVILEGES ON test_db.* TO 'test'@'localhost';\n",
    "\n",
    "-- (선택) 권한 확인\n",
    "SHOW GRANTS FOR 'test'@'localhost';\n",
    "\n",
    "-- DB 사용 선언\n",
    "USE `test_db`;\n",
    "\n",
    "-- users 테이블 생성\n",
    "CREATE TABLE IF NOT EXISTS `users` (\n",
    "  `id`    BIGINT UNSIGNED NOT NULL,\n",
    "  `name`  VARCHAR(50)     NOT NULL,\n",
    "  `email` VARCHAR(255)    NOT NULL,\n",
    "  `age`   INT UNSIGNED    NULL,\n",
    "  PRIMARY KEY (`id`),\n",
    "  UNIQUE KEY `uk_users_email` (`email`)\n",
    ") ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n",
    "\n",
    "-- users의 컬럼 추가\n",
    "INSERT INTO `users` (`id`, `name`, `email`, `age`) VALUES\n",
    "(1, 'Alice',   'alice@example.com',   30),\n",
    "(2, 'Bob',     'bob@example.com',     25),\n",
    "(3, 'Charlie', 'charlie@example.com', 35);\n",
    "\n",
    "-- users 테이블 조회\n",
    "SELECT * FROM users;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe2a5d64-18d1-49fe-be66-4033eda09c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-11 13:49:36,084 INFO sqlalchemy.engine.Engine SELECT DATABASE()\n",
      "2025-09-11 13:49:36,084 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2025-09-11 13:49:36,087 INFO sqlalchemy.engine.Engine SELECT @@sql_mode\n",
      "2025-09-11 13:49:36,087 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2025-09-11 13:49:36,088 INFO sqlalchemy.engine.Engine SELECT @@lower_case_table_names\n",
      "2025-09-11 13:49:36,088 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2025-09-11 13:49:36,089 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-09-11 13:49:36,089 INFO sqlalchemy.engine.Engine SELECT * FROM users\n",
      "2025-09-11 13:49:36,089 INFO sqlalchemy.engine.Engine [generated in 0.00032s] {}\n",
      "2025-09-11 13:49:36,094 INFO sqlalchemy.engine.Engine ROLLBACK\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from llama_index.readers.database import DatabaseReader\n",
    "\n",
    "# MySQL 연결 정보 직접 입력\n",
    "scheme = \"mysql+pymysql\"\n",
    "host = \"localhost\"\n",
    "password = \"test\"\n",
    "port = \"3306\"\n",
    "user = \"test\"\n",
    "dbname = \"test_db\"\n",
    "\n",
    "connection_string = f\"{scheme}://{user}:{password}@{host}:{port}/{dbname}\"\n",
    "engine = create_engine(connection_string, echo=True)\n",
    "reader = DatabaseReader(sql_database=engine)\n",
    "\n",
    "# 데이터 로드\n",
    "query = \"SELECT * FROM users\"\n",
    "documents = reader.load_data(query=query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5e2bd99-92a7-4b80-b46c-73300a8a8a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "ID 929e2cca-7e84-4754-b94a-9c2c1a3c6b3f\n",
      "Row id: 1, name: Alice, email: alice@example.com, age: 30\n",
      "--------------------------------------------------\n",
      "Document 2:\n",
      "ID 7c074100-195f-49a7-93b1-e32131c413f0\n",
      "Row id: 2, name: Bob, email: bob@example.com, age: 25\n",
      "--------------------------------------------------\n",
      "Document 3:\n",
      "ID a99e79d7-36a1-4f53-b8da-eaa44dd32b62\n",
      "Row id: 3, name: Charlie, email: charlie@example.com, age: 35\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# documents 리스트의 각 요소 출력\n",
    "for idx, doc in enumerate(documents):\n",
    "    print(f\"Document {idx + 1}:\")\n",
    "    print(f\"ID {doc.id_}\")\n",
    "    print(f\"Row {doc.text}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74cbcce-07df-441c-8561-4fb69c826997",
   "metadata": {},
   "source": [
    "## 2.3 텍스트 분할"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef50fea-7d89-4535-8395-265c308d2e20",
   "metadata": {},
   "source": [
    "### 2.3.1 문서와 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "262db416-1ab1-4d21-aff3-bfc9ba6769b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc ID: 63729fe5-505a-4922-9788-0618293238de\n",
      "Text: 영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 벌어지는 이야기입니다. 처음에는\n",
      "평화로워 보이지만, 이들의 거짓말이 쌓이며 긴장감이 점점 고조됩니다. 박 사장네 집에는 비밀 지하실이 존재하며, 그곳에는\n",
      "오랫동안 숨어 살던 남자가 있다는 반전이 있습니다. 이 사실을 알게 된 기우네 가족은 예상치 못한 위기를 맞이하게 됩니다.\n",
      "결국 극한 상황에서 벌어지는 사건으로 인해 비극적인 결말로 이어집니다.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "# 텍스트 데이터를 기반으로 문서 생성\n",
    "document_text = \"영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 벌어지는 이야기입니다. 처음에는 평화로워 보이지만, 이들의 거짓말이 쌓이며 긴장감이 점점 고조됩니다. 박 사장네 집에는 비밀 지하실이 존재하며, 그곳에는 오랫동안 숨어 살던 남자가 있다는 반전이 있습니다. 이 사실을 알게 된 기우네 가족은 예상치 못한 위기를 맞이하게 됩니다. 결국 극한 상황에서 벌어지는 사건으로 인해 비극적인 결말로 이어집니다.\"\n",
    "document = Document(text=document_text)\n",
    "\n",
    "# 메타데이터 추가\n",
    "document.metadata = {'author': '영화 해설', 'subject': '기생충 줄거리'}\n",
    "\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6fb6baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/paesir/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /Users/paesir/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/paesir/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "# SSL 인증서 검증 비활성화\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# NLTK 데이터 다운로드\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e04ceb2-43b3-455a-a8de-5017523baf87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "생성된 노드들\n",
      "노드 1: 영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 벌어지는 이야기입니다.\n",
      "   메타데이터: {'type': '영화 줄거리', 'genre': '드라마', 'node_id': 1}\n",
      "노드 2: 처음에는 평화로워 보이지만, 이들의 거짓말이 쌓이며 긴장감이 점점 고조됩니다.\n",
      "   메타데이터: {'type': '영화 줄거리', 'genre': '드라마', 'node_id': 2}\n",
      "노드 3: 박 사장네 집에는 비밀 지하실이 존재하며, 그곳에는 오랫동안 숨어 살던 남자가 있다는 반전이 있습니다.\n",
      "   메타데이터: {'type': '영화 줄거리', 'genre': '드라마', 'node_id': 3}\n",
      "노드 4: 이 사실을 알게 된 기우네 가족은 예상치 못한 위기를 맞이하게 됩니다.\n",
      "   메타데이터: {'type': '영화 줄거리', 'genre': '드라마', 'node_id': 4}\n",
      "노드 5: 결국 극한 상황에서 벌어지는 사건으로 인해 비극적인 결말로 이어집니다.\n",
      "   메타데이터: {'type': '영화 줄거리', 'genre': '드라마', 'node_id': 5}\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "\n",
    "parser = SimpleNodeParser(chunk_size=80, chunk_overlap=0)\n",
    "nodes = parser.get_nodes_from_documents([document])\n",
    "\n",
    "print(\"\\n생성된 노드들\")\n",
    "for idx, node in enumerate(nodes, start=1):\n",
    "    node.metadata = {'type': '영화 줄거리', 'genre': '드라마', 'node_id': idx}\n",
    "    print(f\"노드 {idx}: {node.text}\")\n",
    "    print(f\"   메타데이터: {node.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ad20f75-422a-43e4-afb3-eda1a6839406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "청크 1: 영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 벌어지는 이야기입니다. 처음에는 평화로워 보이지만, 이들의\n",
      "[토큰 개수: 77]\n",
      "\n",
      "청크 2: 거짓말이 쌓이며 긴장감이 점점 고조됩니다. 박 사장네 집에는 비밀 지하실이 존재하며, 그곳에는 오랫동안 숨어 살던 남자가 있다는\n",
      "[토큰 개수: 78]\n",
      "\n",
      "청크 3: 반전이 있습니다. 이 사실을 알게 된 기우네 가족은 예상치 못한 위기를 맞이하게 됩니다. 결국 극한 상황에서 벌어지는 사건으로 인해 비극적인 결말로 이어집니다.\n",
      "[토큰 개수: 80]\n",
      "\n",
      "총 생성된 청크 개수: 3\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "import tiktoken\n",
    "\n",
    "# cl100k_base 토크나이저 로드\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "token_splitter = TokenTextSplitter(chunk_size=80, chunk_overlap=0)\n",
    "\n",
    "chunks = token_splitter.split_text(document_text)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    token_count = len(tokenizer.encode(chunk))  # 각 청크의 토큰 개수 계산\n",
    "    print(f\"청크 {i+1}: {chunk}\\n[토큰 개수: {token_count}]\\n\")\n",
    "\n",
    "print(f\"총 생성된 청크 개수: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72026578-0a3f-48ba-9b1e-01e6a9fe3728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "문서 단위 검색 결과\n",
      "문서 검색 응답: 영화의 반전은 박 사장네 집에 비밀 지하실이 존재하며, 그곳에 오랫동안 숨어 살던 남자가 있다는 사실입니다.\n",
      "- 결과 1: 영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 벌어지는 이야기입니다. 처음에는 평화로워 보이지만, 이들의 거짓말이 쌓이며 긴장감이 점점 고조됩니다. 박 사장네 집에는 비밀 지하실이 존재하며, 그곳에는 오랫동안 숨어 살던 남자가 있다는 반전이 있습니다. 이 사실을 알게 된 기우네 가족은 예상치 못한 위기를 맞이하게 됩니다. 결국 극한 상황에서 벌어지는 사건으로 인해 비극적인 결말로 이어집니다.\n",
      "\n",
      "노드 단위 검색 결과\n",
      "노드 검색 응답: 이 영화의 반전은 박 사장네 집에 비밀 지하실이 존재하며, 그곳에 오랫동안 숨어 살던 남자가 있다는 것입니다.\n",
      "- 결과 노드 1: 박 사장네 집에는 비밀 지하실이 존재하며, 그곳에는 오랫동안 숨어 살던 남자가 있다는 반전이 있습니다.\n",
      "- 결과 노드 2: 결국 극한 상황에서 벌어지는 사건으로 인해 비극적인 결말로 이어집니다.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, Document, Settings\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# 한글 답변 설정\n",
    "api_key = 'OpenAI_Api_Key'\n",
    "# 또는 환경 변수에서 가져오기\n",
    "# api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = OpenAI(api_key=api_key, model=\"gpt-4o\", system_prompt=\"반드시 한국어로 답변하세요.\")\n",
    "embed_model = OpenAIEmbedding(api_key=api_key)\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# 문서 단위 검색을 위한 전체 문서 인덱스 생성\n",
    "new_document = Document(text=document_text)\n",
    "full_doc_index = VectorStoreIndex.from_documents([new_document])\n",
    "\n",
    "# NodeParser를 사용하여 문서를 작은 의미 단위(Node)로 분할\n",
    "# parser = SimpleNodeParser(chunk_size=100, chunk_overlap=0)\n",
    "# nodes = parser.get_nodes_from_documents([document])\n",
    "\n",
    "# 노드 단위 검색을 위한 인덱스 생성\n",
    "node_index = VectorStoreIndex(nodes, llm=llm)\n",
    "\n",
    "# 검색 비교\n",
    "query_text = '이 영화의 반전은 무엇인가요?'\n",
    "\n",
    "## 문서 단위 검색\n",
    "print(\"\\n문서 단위 검색 결과\")\n",
    "doc_query_engine = full_doc_index.as_query_engine()\n",
    "doc_response = doc_query_engine.query(query_text)\n",
    "print(f\"문서 검색 응답: {doc_response.response}\")\n",
    "if doc_response.source_nodes:\n",
    "    for idx, document in enumerate(doc_response.source_nodes, start=1):\n",
    "        print(f\"- 결과 {idx}: {document.node.text}\")\n",
    "\n",
    "## 노드 단위 검색\n",
    "print(\"\\n노드 단위 검색 결과\")\n",
    "node_query_engine = node_index.as_query_engine()\n",
    "node_response = node_query_engine.query(query_text)\n",
    "print(f\"노드 검색 응답: {node_response.response}\")\n",
    "if node_response.source_nodes:\n",
    "    for idx, document in enumerate(node_response.source_nodes, start=1):\n",
    "        print(f\"- 결과 노드 {idx}: {document.node.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e408b1-6994-4563-95a0-32baaff5ac27",
   "metadata": {},
   "source": [
    "### 2.3.2 토큰 단위 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a958acee-d24e-46a3-83f4-ad697a091662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 토큰 기반 분할 결과 ===\n",
      "Chunk 1: 영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 \n",
      "\n",
      "Chunk 2: 취업하면서 벌어지는 이야기입니다. 처음에는 평화로워 보이지만, 이들의 거짓말이 쌓이며 \n",
      "\n",
      "Chunk 3: 쌓이며 긴장감이 점점 고조됩니다. 그러나 이 영화의 반전은 지하실에서 시작됩니다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 벌어지는 이야기입니다. 처음에는 평화로워 보이지만, 이들의 거짓말이 쌓이며 긴장감이 점점 고조됩니다. 그러나 이 영화의 반전은 지하실에서 시작됩니다.\"\n",
    "\n",
    "\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "token_chunks = token_splitter.split_text(sample_text)\n",
    "print(\"=== 토큰 기반 분할 결과 ===\")\n",
    "for i, chunk in enumerate(token_chunks):\n",
    "    print(f\"Chunk {i + 1}:\", chunk.strip(), \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e6fdf1-894d-42e9-b966-a8d18da837b2",
   "metadata": {},
   "source": [
    "### 2.3.3 문장 단위 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9304545-4be1-4048-b9f1-dfbde66f32b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 문장 기반 분할 결과 ===\n",
      "Chunk 1: 영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 \n",
      "\n",
      "Chunk 2: 벌어지는 이야기입니다. \n",
      "\n",
      "Chunk 3: 처음에는 평화로워 보이지만, 이들의 거짓말이 쌓이며 긴장감이 점점 고조됩니다. \n",
      "\n",
      "Chunk 4: 그러나 이 영화의 반전은 지하실에서 시작됩니다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser.text.sentence import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=50, chunk_overlap=0)\n",
    "\n",
    "sentence_chunks = splitter.split_text(sample_text)\n",
    "print(\"=== 문장 기반 분할 결과 ===\")\n",
    "for i, chunk in enumerate(sentence_chunks):\n",
    "    print(f\"Chunk {i + 1}:\", chunk.strip(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b241315-b062-4ac8-a4d2-dceac7c4f98a",
   "metadata": {},
   "source": [
    "### 2.3.4 의미 단위 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d80a240-dcdb-4c6c-b7b3-d946ed2e8183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 의미 기반 분할 결과 ===\n",
      "Chunk 1: 영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 벌어지는 이야기입니다. \n",
      "\n",
      "Chunk 2: 처음에는 평화로워 보이지만, 이들의 거짓말이 쌓이며 긴장감이 점점 고조됩니다. \n",
      "\n",
      "Chunk 3: 그러나 이 영화의 반전은 지하실에서 시작됩니다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "embed_model = OpenAIEmbedding()\n",
    "\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1,\n",
    "    breakpoint_percentile_threshold=95,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "chunks = splitter.sentence_splitter(sample_text)\n",
    "print(\"=== 의미 기반 분할 결과 ===\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i + 1}:\", chunk.strip(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83f3ce71-5ae0-4b03-bf3d-82efa56faeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index-question-gen-openai 0.2.0 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.14.1 which is incompatible.\n",
      "llama-index 0.11.11 requires llama-index-core<0.12.0,>=0.11.10, but you have llama-index-core 0.14.1 which is incompatible.\n",
      "llama-index-program-openai 0.2.0 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.14.1 which is incompatible.\n",
      "llama-index-agent-openai 0.3.4 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.14.1 which is incompatible.\n",
      "llama-index-multi-modal-llms-openai 0.2.3 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.14.1 which is incompatible.\n",
      "llama-index-embeddings-openai 0.2.5 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.14.1 which is incompatible.\n",
      "llama-index-cli 0.3.1 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.14.1 which is incompatible.\n",
      "llama-index-readers-llama-parse 0.3.0 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.14.1 which is incompatible.\n",
      "llama-index-readers-file 0.2.2 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.14.1 which is incompatible.\n",
      "llama-index-readers-database 0.3.0 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.14.1 which is incompatible.\n",
      "llama-index-llms-openai 0.2.16 requires llama-index-core<0.12.0,>=0.11.7, but you have llama-index-core 0.14.1 which is incompatible.\n",
      "llama-index-indices-managed-llama-cloud 0.6.0 requires llama-index-core<0.12.0,>=0.11.13.post1, but you have llama-index-core 0.14.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers -q\n",
    "!pip install llama-index-embeddings-huggingface -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e03d8b04-1173-49b0-9fb9-4ce97a5e22b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paesir/Desktop/llama-index/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 의미 기반 분할 결과 ===\n",
      "Chunk 1: 영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 벌어지는 이야기입니다. \n",
      "\n",
      "Chunk 2: 처음에는 평화로워 보이지만, 이들의 거짓말이 쌓이며 긴장감이 점점 고조됩니다. \n",
      "\n",
      "Chunk 3: 그러나 이 영화의 반전은 지하실에서 시작됩니다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1,\n",
    "    breakpoint_percentile_threshold=95,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "chunks = splitter.sentence_splitter(sample_text)\n",
    "print(\"=== 의미 기반 분할 결과 ===\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i + 1}:\", chunk.strip(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10f7929-12e7-4987-8847-e92cd377c06b",
   "metadata": {},
   "source": [
    "## 2.4 인덱싱"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585e6035-bcf7-4e58-bb88-9c5065cb2f19",
   "metadata": {},
   "source": [
    "### 2.4.2 벡터 저장소 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80af597e-71c7-4756-90a2-252a926352fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader('data')\n",
    "documents = reader.load_data()\n",
    "\n",
    "# Document 객체를 전달하여 인덱스 생성\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e7a4a6-3ced-43b8-874c-be2682b1b8c6",
   "metadata": {},
   "source": [
    "### 2.4.3 Top-K 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "928c6ea4-24b2-49b4-8743-769391dfe143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[검색된 상위 3개 문서]\n",
      "\n",
      "[문서 1]\n",
      "고양이는 물을 충분히 마셔야 합니다. 수분이 부족하면 신장 문제가 발생할 수 있습니다.\n",
      "건식 사료보다 습식 사료가 수분 공급에 도움이 됩니다.\n",
      "\n",
      "[문서 2]\n",
      "고양이는 육식 동물입니다. 주로 고기, 생선, 그리고 가공된 고양이 사료를 먹습니다.\n",
      "특히, 단백질이 풍부한 음식을 선호하며, 탄수화물 섭취는 적은 편입니다.\n",
      "\n",
      "[문서 3]\n",
      "고양이는 초콜릿, 양파, 마늘 같은 음식은 먹으면 안 됩니다.\n",
      "특히, 초콜릿에 포함된 테오브로민 성분은 고양이에게 치명적일 수 있습니다.\n",
      "\n",
      "[답변]\n",
      "고양이에게 수분 공급이 중요한 이유는 수분이 부족할 경우 신장 문제가 발생할 수 있기 때문입니다. 충분한 수분 섭취는 고양이의 건강을 유지하는 데 필수적입니다.\n"
     ]
    }
   ],
   "source": [
    "# 상위 3개의 결과를 반환\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "response = query_engine.query(\"고양이에게 수분 공급이 중요한 이유는?\")\n",
    "\n",
    "print(\"[검색된 상위 3개 문서]\")\n",
    "\n",
    "for idx, node in enumerate(response.source_nodes):\n",
    "    print(f\"\\n[문서 {idx+1}]\\n{node.text}\")\n",
    "print(\"\\n[답변]\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea03d77-c3b7-4aad-a341-bdf467d48f62",
   "metadata": {},
   "source": [
    "## 2.5 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e6b2365-e271-4648-8c3b-3a5888fa367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "435b9b8b-3cf4-43a5-abf0-2389991e3c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# 크로마 클라이언트 초기화\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fcd51fde-b862-4611-aab9-74a1d20ed73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬렉션 생성하기\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be40cb2a-31fa-4f82-a85e-9ba060975020",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index-vector-stores-chroma -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7a6bf4f-9734-4e6d-aab0-fd5bc418eb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "# 크로마를 벡터 저장소로 지정\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6fef2269-7453-4b6b-81c1-a05636c45b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# 문서 리스트 (임의의 예시)\n",
    "documents = [\n",
    "    Document(text=\"Llama2 is a large language model developed by Meta.\"),\n",
    "    Document(text=\"Chroma is an open-source vector store.\")\n",
    "]\n",
    "\n",
    "# 문서를 벡터 스토어 인덱스로 저장\n",
    "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
    "\n",
    "# 저장된 벡터 인덱스 데이터 저장\n",
    "index.storage_context.persist(persist_dir=\"./index_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c44eef47-63e0-439a-8b64-f70fd13f103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eba7014e-2de3-4957-8e11-691c4dc23b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma는 오픈 소스 벡터 저장소입니다.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "# 크로마 클라이언트 초기화\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# 저장된 컬렉션을 다시 가져오기 (또는 생성하기)\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "\n",
    "# 저장된 크로마 벡터 스토어 설정\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# 벡터 스토어 인덱스에 문서를 저장 (데이터 임베딩 후 저장)\n",
    "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
    "\n",
    "# 인덱스 데이터를 로컬 디렉터리에 저장\n",
    "index.storage_context.persist(persist_dir=\"./index_data\")\n",
    "\n",
    "# --- 이후 다시 데이터를 불러오고 쿼리 수행 ---\n",
    "# 크로마 클라이언트 다시 초기화\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# 저장된 컬렉션을 다시 가져오기\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "\n",
    "# 저장된 크로마 벡터 스토어 설정\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# 저장된 벡터 데이터를 이용해 인덱스를 다시 로드\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, storage_context=storage_context\n",
    ")\n",
    "\n",
    "# 쿼리 엔진 생성 및 쿼리 수행\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is Chroma?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3722f76a-ac55-4383-98fb-02e6dcbac8d9",
   "metadata": {},
   "source": [
    "## 2.6 쿼리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c721a238-6561-4679-b304-a8eba84fa029",
   "metadata": {},
   "source": [
    "### 2.6.1 쿼리 엔진"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c548b11-e83a-4ddb-aeba-fda645da783a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "죄송하지만 고객의 개인 정보를 사용하여 맞춤형 이메일을 작성할 수 없습니다. 다른 방법으로 도와드릴 수 있는지 말씀해 주세요.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\n",
    "    \"고객의 개인 정보를 참고하여 맞춤형 이메일을 작성해 주세요.\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691d2ad3-a322-4da4-8396-f5ce5bf9204e",
   "metadata": {},
   "source": [
    "### 2.6.2 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b3f10b9-53ec-4bb9-b593-49af28e54e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=5, # 상위 5개의 결과 반환\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e408d144-6477-46ad-8391-694fa0991348",
   "metadata": {},
   "source": [
    "### 2.6.3 후처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d951a353-b0e1-4b8a-b8f0-a1d847e4596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "postprocessor = SimilarityPostprocessor(similarity_cutoff=0.7)\n",
    "query_engine = index.as_query_engine(node_postprocessors=[postprocessor])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2a0350-4d7b-4dd0-9375-9c6d3e9477e1",
   "metadata": {},
   "source": [
    "### 2.6.4 응답 합성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f7db583-6921-4701-9851-6a8e435c349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# 응답 합성기 설정\n",
    "response_synthesizer = get_response_synthesizer(response_mode=\"compact\")\n",
    "\n",
    "# 쿼리 엔진 구성\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=index.as_retriever(),\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "# 쿼리 실행\n",
    "response = query_engine.query(\"Llama2란?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af210fc-4e3d-4632-bd53-f8c441ad5350",
   "metadata": {},
   "source": [
    "### 2.6.5 커스터마이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e3cc6bf-4638-41b7-a858-0589e9517e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "# 문서를 기반으로 인덱스 생성\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07de41fb-13c2-4c0c-8f10-7e95509eb229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색기 설정 (상위 10개의 유사한 결과 반환)\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6a53bf62-9cfb-4f2d-a776-d765ebd0d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 응답 합성기 설정\n",
    "response_synthesizer = get_response_synthesizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d5724cba-a5c3-4e0e-af8f-3188f4ab85f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 후처리 설정 (유사도 0.7 이상인 노드만 선택)\n",
    "postprocessor = SimilarityPostprocessor(similarity_cutoff=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05f5f1d8-6145-4540-a489-b5db5f6b6a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 쿼리 엔진\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8b98180a-4549-4a7e-80b5-162a2171c992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모나리자 그림은 프랑스 파리에 있는 루브르 박물관에 전시되어 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 쿼리 실행 및 결과 출력\n",
    "response = query_engine.query(\"모나리자 그림은 어디에 전시되어 있나요?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
