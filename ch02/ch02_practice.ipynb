{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05211748",
   "metadata": {},
   "source": [
    "# 파이썬 3.12버전 이용\n",
    "## 3.13 버전은 에러 발생합니다.\n",
    "\n",
    "## 라마인덱스 코어 나 몇몇 pip에서 install 시 Error 발생코드가 나오는데 무시하고 진행하셔도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e64f638-fa34-4ef7-a0ae-e4f3c3f62656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index-readers-database 0.3.0 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.11.23 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index-readers-database 0.3.0 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.11.11 which is incompatible.\n",
      "llama-index-indices-managed-llama-cloud 0.6.0 requires llama-index-core<0.12.0,>=0.11.13.post1, but you have llama-index-core 0.11.11 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install llama-index==0.11.11 -q\n",
    "!pip install llama-index-core==0.11.11 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d511edf9-6b4d-4e7f-88b4-f6829dd72cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: docx2txt in /Users/paesir/Desktop/git/llama-index/.venv/lib/python3.12/site-packages (0.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install docx2txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca605ea-5889-4362-a99f-428cd0f0f349",
   "metadata": {},
   "source": [
    "## 2.2 데이터 로딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4d8a9e-1d9d-4f4e-a1a2-ea3ea9d87fe2",
   "metadata": {},
   "source": [
    "### 2.2.1 데이터 리더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52e12c8c-a9c1-467d-83f9-5075eb69dfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"sample_docs\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68b5a7cf-da7a-4f49-a8f7-1f364f82e688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "This is the content of file 1.\n",
      "\n",
      "\n",
      "Document 2:\n",
      "This is the content of file 2.\n",
      "\n",
      "\n",
      "Document 3:\n",
      "This is the content of ﬁle 3. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Document 리스트의 각 요소에 접근하여 내용 출력\n",
    "for i, document in enumerate(documents):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(document.text)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9c2ef93-2743-4f6c-84a8-1577b9d993e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\n",
    "    \"sample_docs\", recursive=True).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "592bdffb-8901-4244-8d02-385fa979e056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "This is the content of file 1.\n",
      "\n",
      "\n",
      "Document 2:\n",
      "This is the content of file 2.\n",
      "\n",
      "\n",
      "Document 3:\n",
      "This is the content of ﬁle 3. \n",
      "\n",
      "\n",
      "Document 4:\n",
      "This is the content of file 4.\n",
      "\n",
      "\n",
      "Document 5:\n",
      "This is the content of file 5.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Document 리스트의 각 요소에 접근하여 내용 출력\n",
    "for i, document in enumerate(documents):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(document.text)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "085eea22-bf75-4d39-bf44-85f7588b137c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\n",
    "    \"sample_docs\",\n",
    "    required_exts=[\".txt\", \".pdf\"],\n",
    "    recursive=True).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd589fcd-bafb-456e-9c45-a2190a2e070b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "This is the content of file 1.\n",
      "\n",
      "\n",
      "Document 2:\n",
      "This is the content of ﬁle 3. \n",
      "\n",
      "\n",
      "Document 3:\n",
      "This is the content of file 4.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Document 리스트의 각 요소에 접근하여 내용 출력\n",
    "for i, document in enumerate(documents):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(document.text)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29a7554c-8a98-47cf-aa07-eb1e191efc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 metadata:\n",
      "{'file_path': '/Users/paesir/Desktop/git/llama-index/ch02/sample_docs/file1.txt', 'file_name': 'file1.txt', 'file_type': 'text/plain', 'file_size': 30, 'creation_date': '2025-09-12', 'last_modified_date': '2025-09-12'}\n",
      "\n",
      "\n",
      "Document 2 metadata:\n",
      "{'page_label': '1', 'file_name': 'file3.pdf', 'file_path': '/Users/paesir/Desktop/git/llama-index/ch02/sample_docs/file3.pdf', 'file_type': 'application/pdf', 'file_size': 9283, 'creation_date': '2025-09-12', 'last_modified_date': '2025-09-12'}\n",
      "\n",
      "\n",
      "Document 3 metadata:\n",
      "{'file_path': '/Users/paesir/Desktop/git/llama-index/ch02/sample_docs/sub_sample_docs/file4.txt', 'file_name': 'file4.txt', 'file_type': 'text/plain', 'file_size': 30, 'creation_date': '2025-09-12', 'last_modified_date': '2025-09-12'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# document 리스트의 각 요소에 접근하여 메타데이터를 출력\n",
    "for i, document in enumerate(documents):\n",
    "    print(f\"Document {i+1} metadata:\")\n",
    "    print(document.metadata)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9a7247-09a8-4d1b-8713-d735e99491a5",
   "metadata": {},
   "source": [
    "### 2.2.2 데이터 커넥터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc393c4f-234e-47f1-9b76-573cc05c00da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index-question-gen-openai 0.2.0 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
      "llama-index 0.11.11 requires llama-index-core<0.12.0,>=0.11.10, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
      "llama-index-program-openai 0.2.0 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
      "llama-index-agent-openai 0.3.4 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
      "llama-index-multi-modal-llms-openai 0.2.3 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
      "llama-index-embeddings-openai 0.2.5 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
      "llama-index-cli 0.3.1 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
      "llama-index-readers-llama-parse 0.3.0 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
      "llama-index-readers-file 0.2.2 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
      "llama-index-llms-openai 0.2.16 requires llama-index-core<0.12.0,>=0.11.7, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
      "llama-index-indices-managed-llama-cloud 0.6.0 requires llama-index-core<0.12.0,>=0.11.13.post1, but you have llama-index-core 0.12.52.post1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install llama-index-readers-database==0.3.0 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9ed0e00-2c38-4383-8b74-84d88071868d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymysql in /Users/paesir/Desktop/git/llama-index/.venv/lib/python3.12/site-packages (1.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8d4ad44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cryptography in /Users/paesir/Desktop/git/llama-index/.venv/lib/python3.12/site-packages (45.0.7)\n",
      "Requirement already satisfied: cffi>=1.14 in /Users/paesir/Desktop/git/llama-index/.venv/lib/python3.12/site-packages (from cryptography) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /Users/paesir/Desktop/git/llama-index/.venv/lib/python3.12/site-packages (from cffi>=1.14->cryptography) (2.23)\n"
     ]
    }
   ],
   "source": [
    "  !pip install cryptography"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ecbf2c",
   "metadata": {},
   "source": [
    "## MySql 테이블 및 정보 추가 쿼리\n",
    "\n",
    "```sql\n",
    "-- 1) 데이터베이스 생성\n",
    "CREATE DATABASE IF NOT EXISTS test_db\n",
    "  DEFAULT CHARACTER SET utf8mb4\n",
    "  DEFAULT COLLATE utf8mb4_0900_ai_ci;\n",
    "\n",
    "-- 2) 로컬 전용 사용자 생성\n",
    "CREATE USER IF NOT EXISTS 'test'@'localhost' IDENTIFIED BY 'test';\n",
    "\n",
    "-- 3) 해당 DB에 대한 권한 부여\n",
    "GRANT ALL PRIVILEGES ON test_db.* TO 'test'@'localhost';\n",
    "\n",
    "-- (선택) 권한 확인\n",
    "SHOW GRANTS FOR 'test'@'localhost';\n",
    "\n",
    "-- DB 사용 선언\n",
    "USE `test_db`;\n",
    "\n",
    "-- users 테이블 생성\n",
    "CREATE TABLE IF NOT EXISTS `users` (\n",
    "  `id`    BIGINT UNSIGNED NOT NULL,\n",
    "  `name`  VARCHAR(50)     NOT NULL,\n",
    "  `email` VARCHAR(255)    NOT NULL,\n",
    "  `age`   INT UNSIGNED    NULL,\n",
    "  PRIMARY KEY (`id`),\n",
    "  UNIQUE KEY `uk_users_email` (`email`)\n",
    ") ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n",
    "\n",
    "-- users의 컬럼 추가\n",
    "INSERT INTO `users` (`id`, `name`, `email`, `age`) VALUES\n",
    "(1, 'Alice',   'alice@example.com',   30),\n",
    "(2, 'Bob',     'bob@example.com',     25),\n",
    "(3, 'Charlie', 'charlie@example.com', 35);\n",
    "\n",
    "-- users 테이블 조회\n",
    "SELECT * FROM users;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe2a5d64-18d1-49fe-be66-4033eda09c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-13 10:56:48,594 INFO sqlalchemy.engine.Engine SELECT DATABASE()\n",
      "2025-09-13 10:56:48,595 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2025-09-13 10:56:48,596 INFO sqlalchemy.engine.Engine SELECT @@sql_mode\n",
      "2025-09-13 10:56:48,596 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2025-09-13 10:56:48,596 INFO sqlalchemy.engine.Engine SELECT @@lower_case_table_names\n",
      "2025-09-13 10:56:48,596 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2025-09-13 10:56:48,597 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-09-13 10:56:48,597 INFO sqlalchemy.engine.Engine SELECT * FROM users\n",
      "2025-09-13 10:56:48,597 INFO sqlalchemy.engine.Engine [generated in 0.00031s] {}\n",
      "2025-09-13 10:56:48,598 INFO sqlalchemy.engine.Engine ROLLBACK\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from llama_index.readers.database import DatabaseReader\n",
    "\n",
    "# MySQL 연결 정보 직접 입력\n",
    "scheme = \"mysql+pymysql\"\n",
    "host = \"localhost\"\n",
    "password = \"test\"\n",
    "port = \"3306\"\n",
    "user = \"test\"\n",
    "dbname = \"test_db\"\n",
    "\n",
    "connection_string = f\"{scheme}://{user}:{password}@{host}:{port}/{dbname}\"\n",
    "engine = create_engine(connection_string, echo=True)\n",
    "reader = DatabaseReader(sql_database=engine)\n",
    "\n",
    "# 데이터 로드\n",
    "query = \"SELECT * FROM users\"\n",
    "documents = reader.load_data(query=query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5e2bd99-92a7-4b80-b46c-73300a8a8a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "ID ff5fea19-aa25-4134-bf94-ff780bff2a0c\n",
      "Row id: 1, name: Alice, email: alice@example.com, age: 30\n",
      "--------------------------------------------------\n",
      "Document 2:\n",
      "ID c517ab56-619a-447a-95a9-dad3b6734b0e\n",
      "Row id: 2, name: Bob, email: bob@example.com, age: 25\n",
      "--------------------------------------------------\n",
      "Document 3:\n",
      "ID c774e402-a6d7-441e-9597-5bd5c6dc110a\n",
      "Row id: 3, name: Charlie, email: charlie@example.com, age: 35\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# documents 리스트의 각 요소 출력\n",
    "for idx, doc in enumerate(documents):\n",
    "    print(f\"Document {idx + 1}:\")\n",
    "    print(f\"ID {doc.id_}\")\n",
    "    print(f\"Row {doc.text}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74cbcce-07df-441c-8561-4fb69c826997",
   "metadata": {},
   "source": [
    "## 2.3 텍스트 분할"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef50fea-7d89-4535-8395-265c308d2e20",
   "metadata": {},
   "source": [
    "### 2.3.1 문서와 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "262db416-1ab1-4d21-aff3-bfc9ba6769b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc ID: acc673b4-fd61-4cb0-b1c7-21fa99ac361d\n",
      "Text: 영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 벌어지는 이야기입니다. 처음에는\n",
      "평화로워 보이지만, 이들의 거짓말이 쌓이며 긴장감이 점점 고조됩니다. 박 사장네 집에는 비밀 지하실이 존재하며, 그곳에는\n",
      "오랫동안 숨어 살던 남자가 있다는 반전이 있습니다. 이 사실을 알게 된 기우네 가족은 예상치 못한 위기를 맞이하게 됩니다.\n",
      "결국 극한 상황에서 벌어지는 사건으로 인해 비극적인 결말로 이어집니다.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "# 텍스트 데이터를 기반으로 문서 생성\n",
    "document_text = \"영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 벌어지는 이야기입니다. 처음에는 평화로워 보이지만, 이들의 거짓말이 쌓이며 긴장감이 점점 고조됩니다. 박 사장네 집에는 비밀 지하실이 존재하며, 그곳에는 오랫동안 숨어 살던 남자가 있다는 반전이 있습니다. 이 사실을 알게 된 기우네 가족은 예상치 못한 위기를 맞이하게 됩니다. 결국 극한 상황에서 벌어지는 사건으로 인해 비극적인 결말로 이어집니다.\"\n",
    "document = Document(text=document_text)\n",
    "\n",
    "# 메타데이터 추가\n",
    "document.metadata = {'author': '영화 해설', 'subject': '기생충 줄거리'}\n",
    "\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6fb6baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/paesir/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/paesir/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/paesir/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "# SSL 인증서 검증 비활성화\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# NLTK 데이터 다운로드\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e04ceb2-43b3-455a-a8de-5017523baf87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "생성된 노드들\n",
      "노드 1: 영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 벌어지는 이야기입니다.\n",
      "   메타데이터: {'type': '영화 줄거리', 'genre': '드라마', 'node_id': 1}\n",
      "노드 2: 처음에는 평화로워 보이지만, 이들의 거짓말이 쌓이며 긴장감이 점점 고조됩니다.\n",
      "   메타데이터: {'type': '영화 줄거리', 'genre': '드라마', 'node_id': 2}\n",
      "노드 3: 박 사장네 집에는 비밀 지하실이 존재하며, 그곳에는 오랫동안 숨어 살던 남자가 있다는 반전이 있습니다.\n",
      "   메타데이터: {'type': '영화 줄거리', 'genre': '드라마', 'node_id': 3}\n",
      "노드 4: 이 사실을 알게 된 기우네 가족은 예상치 못한 위기를 맞이하게 됩니다.\n",
      "   메타데이터: {'type': '영화 줄거리', 'genre': '드라마', 'node_id': 4}\n",
      "노드 5: 결국 극한 상황에서 벌어지는 사건으로 인해 비극적인 결말로 이어집니다.\n",
      "   메타데이터: {'type': '영화 줄거리', 'genre': '드라마', 'node_id': 5}\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "\n",
    "parser = SimpleNodeParser(chunk_size=80, chunk_overlap=0)\n",
    "nodes = parser.get_nodes_from_documents([document])\n",
    "\n",
    "print(\"\\n생성된 노드들\")\n",
    "for idx, node in enumerate(nodes, start=1):\n",
    "    node.metadata = {'type': '영화 줄거리', 'genre': '드라마', 'node_id': idx}\n",
    "    print(f\"노드 {idx}: {node.text}\")\n",
    "    print(f\"   메타데이터: {node.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ad20f75-422a-43e4-afb3-eda1a6839406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "청크 1: 영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 벌어지는 이야기입니다. 처음에는 평화로워 보이지만, 이들의\n",
      "[토큰 개수: 77]\n",
      "\n",
      "청크 2: 거짓말이 쌓이며 긴장감이 점점 고조됩니다. 박 사장네 집에는 비밀 지하실이 존재하며, 그곳에는 오랫동안 숨어 살던 남자가 있다는\n",
      "[토큰 개수: 78]\n",
      "\n",
      "청크 3: 반전이 있습니다. 이 사실을 알게 된 기우네 가족은 예상치 못한 위기를 맞이하게 됩니다. 결국 극한 상황에서 벌어지는 사건으로 인해 비극적인 결말로 이어집니다.\n",
      "[토큰 개수: 80]\n",
      "\n",
      "총 생성된 청크 개수: 3\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "import tiktoken\n",
    "\n",
    "# cl100k_base 토크나이저 로드\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "token_splitter = TokenTextSplitter(chunk_size=80, chunk_overlap=0)\n",
    "\n",
    "chunks = token_splitter.split_text(document_text)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    token_count = len(tokenizer.encode(chunk))  # 각 청크의 토큰 개수 계산\n",
    "    print(f\"청크 {i+1}: {chunk}\\n[토큰 개수: {token_count}]\\n\")\n",
    "\n",
    "print(f\"총 생성된 청크 개수: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72026578-0a3f-48ba-9b1e-01e6a9fe3728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "문서 단위 검색 결과\n",
      "문서 검색 응답: 이 영화의 반전은 박 사장네 집에 비밀 지하실이 존재하며, 그곳에 오랫동안 숨어 살던 남자가 있다는 사실입니다. 이 사실을 알게 된 기우네 가족은 예상치 못한 위기를 맞이하게 됩니다.\n",
      "- 결과 1: 영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 벌어지는 이야기입니다. 처음에는 평화로워 보이지만, 이들의 거짓말이 쌓이며 긴장감이 점점 고조됩니다. 박 사장네 집에는 비밀 지하실이 존재하며, 그곳에는 오랫동안 숨어 살던 남자가 있다는 반전이 있습니다. 이 사실을 알게 된 기우네 가족은 예상치 못한 위기를 맞이하게 됩니다. 결국 극한 상황에서 벌어지는 사건으로 인해 비극적인 결말로 이어집니다.\n",
      "\n",
      "노드 단위 검색 결과\n",
      "노드 검색 응답: 이 영화의 반전은 박 사장네 집에 비밀 지하실이 존재하며, 그곳에 오랫동안 숨어 살던 남자가 있다는 점입니다.\n",
      "- 결과 노드 1: 박 사장네 집에는 비밀 지하실이 존재하며, 그곳에는 오랫동안 숨어 살던 남자가 있다는 반전이 있습니다.\n",
      "- 결과 노드 2: 결국 극한 상황에서 벌어지는 사건으로 인해 비극적인 결말로 이어집니다.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, Document, Settings\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# 한글 답변 설정\n",
    "api_key = 'your-api-key-here'\n",
    "# 또는 환경 변수에서 가져오기\n",
    "# api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = OpenAI(api_key=api_key, model=\"gpt-4o-mini\", system_prompt=\"반드시 한국어로 답변하세요.\")\n",
    "embed_model = OpenAIEmbedding(api_key=api_key)\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# 문서 단위 검색을 위한 전체 문서 인덱스 생성\n",
    "new_document = Document(text=document_text)\n",
    "full_doc_index = VectorStoreIndex.from_documents([new_document])\n",
    "\n",
    "# NodeParser를 사용하여 문서를 작은 의미 단위(Node)로 분할\n",
    "# parser = SimpleNodeParser(chunk_size=100, chunk_overlap=0)\n",
    "# nodes = parser.get_nodes_from_documents([document])\n",
    "\n",
    "# 노드 단위 검색을 위한 인덱스 생성\n",
    "node_index = VectorStoreIndex(nodes, llm=llm)\n",
    "\n",
    "# 검색 비교\n",
    "query_text = '이 영화의 반전은 무엇인가요?'\n",
    "\n",
    "## 문서 단위 검색\n",
    "print(\"\\n문서 단위 검색 결과\")\n",
    "doc_query_engine = full_doc_index.as_query_engine()\n",
    "doc_response = doc_query_engine.query(query_text)\n",
    "print(f\"문서 검색 응답: {doc_response.response}\")\n",
    "if doc_response.source_nodes:\n",
    "    for idx, document in enumerate(doc_response.source_nodes, start=1):\n",
    "        print(f\"- 결과 {idx}: {document.node.text}\")\n",
    "\n",
    "## 노드 단위 검색\n",
    "print(\"\\n노드 단위 검색 결과\")\n",
    "node_query_engine = node_index.as_query_engine()\n",
    "node_response = node_query_engine.query(query_text)\n",
    "print(f\"노드 검색 응답: {node_response.response}\")\n",
    "if node_response.source_nodes:\n",
    "    for idx, document in enumerate(node_response.source_nodes, start=1):\n",
    "        print(f\"- 결과 노드 {idx}: {document.node.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e408b1-6994-4563-95a0-32baaff5ac27",
   "metadata": {},
   "source": [
    "### 2.3.2 토큰 단위 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a958acee-d24e-46a3-83f4-ad697a091662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 토큰 기반 분할 결과 ===\n",
      "Chunk 1: 영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 \n",
      "\n",
      "Chunk 2: 취업하면서 벌어지는 이야기입니다. 처음에는 평화로워 보이지만, 이들의 거짓말이 쌓이며 \n",
      "\n",
      "Chunk 3: 쌓이며 긴장감이 점점 고조됩니다. 그러나 이 영화의 반전은 지하실에서 시작됩니다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 벌어지는 이야기입니다. 처음에는 평화로워 보이지만, 이들의 거짓말이 쌓이며 긴장감이 점점 고조됩니다. 그러나 이 영화의 반전은 지하실에서 시작됩니다.\"\n",
    "\n",
    "\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "token_chunks = token_splitter.split_text(sample_text)\n",
    "print(\"=== 토큰 기반 분할 결과 ===\")\n",
    "for i, chunk in enumerate(token_chunks):\n",
    "    print(f\"Chunk {i + 1}:\", chunk.strip(), \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e6fdf1-894d-42e9-b966-a8d18da837b2",
   "metadata": {},
   "source": [
    "### 2.3.3 문장 단위 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9304545-4be1-4048-b9f1-dfbde66f32b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 문장 기반 분할 결과 ===\n",
      "Chunk 1: 영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 \n",
      "\n",
      "Chunk 2: 벌어지는 이야기입니다. \n",
      "\n",
      "Chunk 3: 처음에는 평화로워 보이지만, 이들의 거짓말이 쌓이며 긴장감이 점점 고조됩니다. \n",
      "\n",
      "Chunk 4: 그러나 이 영화의 반전은 지하실에서 시작됩니다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser.text.sentence import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=50, chunk_overlap=0)\n",
    "\n",
    "sentence_chunks = splitter.split_text(sample_text)\n",
    "print(\"=== 문장 기반 분할 결과 ===\")\n",
    "for i, chunk in enumerate(sentence_chunks):\n",
    "    print(f\"Chunk {i + 1}:\", chunk.strip(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b241315-b062-4ac8-a4d2-dceac7c4f98a",
   "metadata": {},
   "source": [
    "### 2.3.4 의미 단위 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d80a240-dcdb-4c6c-b7b3-d946ed2e8183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 의미 기반 분할 결과 ===\n",
      "Chunk 1: 영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 벌어지는 이야기입니다. \n",
      "\n",
      "Chunk 2: 처음에는 평화로워 보이지만, 이들의 거짓말이 쌓이며 긴장감이 점점 고조됩니다. \n",
      "\n",
      "Chunk 3: 그러나 이 영화의 반전은 지하실에서 시작됩니다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "embed_model = OpenAIEmbedding()\n",
    "\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1,\n",
    "    breakpoint_percentile_threshold=95,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "chunks = splitter.sentence_splitter(sample_text)\n",
    "print(\"=== 의미 기반 분할 결과 ===\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i + 1}:\", chunk.strip(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83f3ce71-5ae0-4b03-bf3d-82efa56faeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index-question-gen-openai 0.2.0 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.14.1 which is incompatible.\n",
      "llama-index 0.11.11 requires llama-index-core<0.12.0,>=0.11.10, but you have llama-index-core 0.14.1 which is incompatible.\n",
      "llama-index-program-openai 0.2.0 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.14.1 which is incompatible.\n",
      "llama-index-agent-openai 0.3.4 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.14.1 which is incompatible.\n",
      "llama-index-multi-modal-llms-openai 0.2.3 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.14.1 which is incompatible.\n",
      "llama-index-embeddings-openai 0.2.5 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.14.1 which is incompatible.\n",
      "llama-index-cli 0.3.1 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.14.1 which is incompatible.\n",
      "llama-index-readers-llama-parse 0.3.0 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.14.1 which is incompatible.\n",
      "llama-index-readers-file 0.2.2 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.14.1 which is incompatible.\n",
      "llama-index-readers-database 0.3.0 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.14.1 which is incompatible.\n",
      "llama-index-llms-openai 0.2.16 requires llama-index-core<0.12.0,>=0.11.7, but you have llama-index-core 0.14.1 which is incompatible.\n",
      "llama-index-indices-managed-llama-cloud 0.6.0 requires llama-index-core<0.12.0,>=0.11.13.post1, but you have llama-index-core 0.14.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers -q\n",
    "!pip install llama-index-embeddings-huggingface -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e03d8b04-1173-49b0-9fb9-4ce97a5e22b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paesir/Desktop/git/llama-index/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 의미 기반 분할 결과 ===\n",
      "Chunk 1: 영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 벌어지는 이야기입니다. \n",
      "\n",
      "Chunk 2: 처음에는 평화로워 보이지만, 이들의 거짓말이 쌓이며 긴장감이 점점 고조됩니다. \n",
      "\n",
      "Chunk 3: 그러나 이 영화의 반전은 지하실에서 시작됩니다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1,\n",
    "    breakpoint_percentile_threshold=95,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "chunks = splitter.sentence_splitter(sample_text)\n",
    "print(\"=== 의미 기반 분할 결과 ===\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i + 1}:\", chunk.strip(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10f7929-12e7-4987-8847-e92cd377c06b",
   "metadata": {},
   "source": [
    "## 2.4 인덱싱"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585e6035-bcf7-4e58-bb88-9c5065cb2f19",
   "metadata": {},
   "source": [
    "### 2.4.2 벡터 저장소 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80af597e-71c7-4756-90a2-252a926352fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader('data')\n",
    "documents = reader.load_data()\n",
    "\n",
    "# Document 객체를 전달하여 인덱스 생성\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e7a4a6-3ced-43b8-874c-be2682b1b8c6",
   "metadata": {},
   "source": [
    "### 2.4.3 Top-K 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "928c6ea4-24b2-49b4-8743-769391dfe143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[검색된 상위 3개 문서]\n",
      "\n",
      "[문서 1]\n",
      "고양이는 물을 충분히 마셔야 합니다. 수분이 부족하면 신장 문제가 발생할 수 있습니다.\n",
      "건식 사료보다 습식 사료가 수분 공급에 도움이 됩니다.\n",
      "\n",
      "[문서 2]\n",
      "고양이는 육식 동물입니다. 주로 고기, 생선, 그리고 가공된 고양이 사료를 먹습니다.\n",
      "특히, 단백질이 풍부한 음식을 선호하며, 탄수화물 섭취는 적은 편입니다.\n",
      "\n",
      "[문서 3]\n",
      "고양이는 초콜릿, 양파, 마늘 같은 음식은 먹으면 안 됩니다.\n",
      "특히, 초콜릿에 포함된 테오브로민 성분은 고양이에게 치명적일 수 있습니다.\n",
      "\n",
      "[답변]\n",
      "고양이에게 수분 공급이 중요한 이유는 수분이 부족할 경우 신장 문제가 발생할 수 있기 때문입니다. 충분한 수분 섭취는 건강을 유지하는 데 필수적입니다.\n"
     ]
    }
   ],
   "source": [
    "# 상위 3개의 결과를 반환\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "response = query_engine.query(\"고양이에게 수분 공급이 중요한 이유는?\")\n",
    "\n",
    "print(\"[검색된 상위 3개 문서]\")\n",
    "\n",
    "for idx, node in enumerate(response.source_nodes):\n",
    "    print(f\"\\n[문서 {idx+1}]\\n{node.text}\")\n",
    "print(\"\\n[답변]\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea03d77-c3b7-4aad-a341-bdf467d48f62",
   "metadata": {},
   "source": [
    "## 2.5 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e6b2365-e271-4648-8c3b-3a5888fa367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "435b9b8b-3cf4-43a5-abf0-2389991e3c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# 크로마 클라이언트 초기화\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fcd51fde-b862-4611-aab9-74a1d20ed73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬렉션 생성하기\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be40cb2a-31fa-4f82-a85e-9ba060975020",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index-vector-stores-chroma -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7a6bf4f-9734-4e6d-aab0-fd5bc418eb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "# 크로마를 벡터 저장소로 지정\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6fef2269-7453-4b6b-81c1-a05636c45b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# 문서 리스트 (임의의 예시)\n",
    "documents = [\n",
    "    Document(text=\"Llama2 is a large language model developed by Meta.\"),\n",
    "    Document(text=\"Chroma is an open-source vector store.\")\n",
    "]\n",
    "\n",
    "# 문서를 벡터 스토어 인덱스로 저장\n",
    "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
    "\n",
    "# 저장된 벡터 인덱스 데이터 저장\n",
    "index.storage_context.persist(persist_dir=\"./index_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c44eef47-63e0-439a-8b64-f70fd13f103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eba7014e-2de3-4957-8e11-691c4dc23b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma는 오픈 소스 벡터 저장소입니다.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "# 크로마 클라이언트 초기화\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# 저장된 컬렉션을 다시 가져오기 (또는 생성하기)\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "\n",
    "# 저장된 크로마 벡터 스토어 설정\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# 벡터 스토어 인덱스에 문서를 저장 (데이터 임베딩 후 저장)\n",
    "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
    "\n",
    "# 인덱스 데이터를 로컬 디렉터리에 저장\n",
    "index.storage_context.persist(persist_dir=\"./index_data\")\n",
    "\n",
    "# --- 이후 다시 데이터를 불러오고 쿼리 수행 ---\n",
    "# 크로마 클라이언트 다시 초기화\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# 저장된 컬렉션을 다시 가져오기\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "\n",
    "# 저장된 크로마 벡터 스토어 설정\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# 저장된 벡터 데이터를 이용해 인덱스를 다시 로드\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, storage_context=storage_context\n",
    ")\n",
    "\n",
    "# 쿼리 엔진 생성 및 쿼리 수행\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is Chroma?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3722f76a-ac55-4383-98fb-02e6dcbac8d9",
   "metadata": {},
   "source": [
    "## 2.6 쿼리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c721a238-6561-4679-b304-a8eba84fa029",
   "metadata": {},
   "source": [
    "### 2.6.1 쿼리 엔진"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c548b11-e83a-4ddb-aeba-fda645da783a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "고객님, 안녕하세요!\n",
      "\n",
      "저희 서비스를 이용해 주셔서 감사합니다. 고객님의 관심사와 선호도를 바탕으로 맞춤형 제안을 드리고자 합니다. 최근에 고객님께서 관심을 보이신 제품에 대해 특별 할인 혜택을 준비했습니다. \n",
      "\n",
      "또한, 고객님의 구매 이력을 고려하여 추천드리는 상품 목록을 아래에 첨부하였습니다. 추가로 궁금하신 점이나 도움이 필요하시면 언제든지 문의해 주세요.\n",
      "\n",
      "감사합니다!\n",
      "\n",
      "[추천 상품 목록]  \n",
      "1. 상품 A  \n",
      "2. 상품 B  \n",
      "3. 상품 C  \n",
      "\n",
      "좋은 하루 되세요!\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\n",
    "    \"고객의 개인 정보를 참고하여 맞춤형 이메일을 작성해 주세요.\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691d2ad3-a322-4da4-8396-f5ce5bf9204e",
   "metadata": {},
   "source": [
    "### 2.6.2 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b3f10b9-53ec-4bb9-b593-49af28e54e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=5, # 상위 5개의 결과 반환\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e408d144-6477-46ad-8391-694fa0991348",
   "metadata": {},
   "source": [
    "### 2.6.3 후처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d951a353-b0e1-4b8a-b8f0-a1d847e4596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "postprocessor = SimilarityPostprocessor(similarity_cutoff=0.7)\n",
    "query_engine = index.as_query_engine(node_postprocessors=[postprocessor])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2a0350-4d7b-4dd0-9375-9c6d3e9477e1",
   "metadata": {},
   "source": [
    "### 2.6.4 응답 합성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f7db583-6921-4701-9851-6a8e435c349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# 응답 합성기 설정\n",
    "response_synthesizer = get_response_synthesizer(response_mode=\"compact\")\n",
    "\n",
    "# 쿼리 엔진 구성\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=index.as_retriever(),\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "# 쿼리 실행\n",
    "response = query_engine.query(\"Llama2란?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af210fc-4e3d-4632-bd53-f8c441ad5350",
   "metadata": {},
   "source": [
    "### 2.6.5 커스터마이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e3cc6bf-4638-41b7-a858-0589e9517e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "# 문서를 기반으로 인덱스 생성\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "07de41fb-13c2-4c0c-8f10-7e95509eb229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색기 설정 (상위 10개의 유사한 결과 반환)\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a53bf62-9cfb-4f2d-a776-d765ebd0d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 응답 합성기 설정\n",
    "response_synthesizer = get_response_synthesizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d5724cba-a5c3-4e0e-af8f-3188f4ab85f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 후처리 설정 (유사도 0.7 이상인 노드만 선택)\n",
    "postprocessor = SimilarityPostprocessor(similarity_cutoff=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "05f5f1d8-6145-4540-a489-b5db5f6b6a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 쿼리 엔진\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8b98180a-4549-4a7e-80b5-162a2171c992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모나리자 그림은 프랑스 파리의 루브르 박물관에 전시되어 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 쿼리 실행 및 결과 출력\n",
    "response = query_engine.query(\"모나리자 그림은 어디에 전시되어 있나요?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22f5785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
